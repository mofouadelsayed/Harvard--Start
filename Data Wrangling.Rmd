---
title: "Data Wrangling"
author: "MO"
date: "30/03/2021"
output: html_document
---
```{r include=FALSE}
library(tidyverse)
library(dslabs)
```

- Spreadsheet files have data stored in rows & columns. If that spreadsheet is opened with a normal text editor the "ROWS" will be separated by "Enter" & the columns will be separated by "," or ";" or " ". It may either also contain a header or not. NOTE: Excel sheets can't be viewed with a text editor.

- To know my current working directory in R `getwd()` & to change the working directory `setwd()`. NOTE: it's always better to provide a full path of the directory i want to change into because by default, if i don't do that, the system will look for files in my current working directory. Ex: `setwd("~/Desktop/HarvardX Data Science/6. Data Wrangling")`
```{r}
# For Example: to know the directory in which the "dslabs" package was installed & what files are in that directory i can do the following
path<- system.file("extdata", package = "dslabs")
path
list.files(path)
```

- Now lets say i want to copy one of the above files to my working directory. I will define the file name i want to copy, then i will define the full path it's located in then i'll copy all that into my working directory. Then i can double check if the file was actually copied
```{r}
filename<- "murders.csv"
fullpath<- file.path(path, filename)
fullpath
file.copy(fullpath, getwd())
file.exists(filename)
```

- Tidyverse package includes functions to read data from Excel sheets directly. They are as follows:
1. `read_excel`: auto detects the excel sheet format either "xls" or "xlsx"
2. `read_xls`: reads files with "xls" format
3. `read_xlsx`: reads files with "xlsx" format
*NOTE:* The sheet function in the above functions specify the sheet to read.

- I can use `read_lines("murders.csv", n_max=3) to read the first 3 lines in that file to know to double check if it has a header & if it's comma or semicolon separated.

- Since i'm trying to import or read a "csv" file, i should use the function as below then i can normally use the `head()` function to check the first 6 lines
```{r}
dat<- read_csv(filename)
```
**OR**
```{r}
dat<- read_csv(fullpath)
class(dat)
```
```{r}
head(dat)
```

- All the above read functions are available in base R as well, but they all use "." instead of "_" in the function name & when the data is imported as data frames as opposed to tibbles in the tidyverse functions. and the strings are converted to factors.
```{r}
dat2<- read.csv(filename)
class(dat2)
```

- To import files from the internet i have 2 options. I can either import or read the file directly from the "url" or i can download a copy on my computer.
```{r eval=FALSE, include=FALSE}
url <- "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
dat <- read_csv(url)                              # To read the file directly from the url
download.file(url, "murders.csv")                 # To download the file to my computer
```

- Plotting data as we used to do in the "Visualization" module worked so seamlessly because data was in "tidy" format, which means it was suitable for plotting because it was edited from it's raw format in a format that makes it easy to plot. For Example: every data point was in a different row, there was a header, etc.. below we can examine the raw data of the "gapminder" dataset.
```{r}
path <- system.file("extdata", package="dslabs")
filename <- file.path(path,  "fertility-two-countries-example.csv")
wide_data <- read_csv(filename)
select(wide_data, country, `1960`:`1967`)         # To check the first 9 columns
```

- Now we need to reshape the "Raw" or "wide" data into a "tidy" format so that we can work with it in R. there are several functions in the "tidyr" package which is a part of the "tidyverse" package already that can do that. below we will explore some functions of that package.

- First function we can use is the `gather()` function which will help us group all the year columns we have under one column & will add the relevant fertility data under those columns. This function takes 4 arguments `gather(1, 2, 3, 4)`. The third argument will be the columns i want to group under one column or title, The first argument will be that title or column name i want to group all other columns under & finally the second argument will be the data i want to include in that table. 
```{r}
new_tidy_data<- wide_data %>% gather(year, fertility, '1960' : '2015')
head(new_tidy_data)

# Since the ONLY column that was not gathered from the wide_data in the example above was the country,i can alternatively write the code like this. Both give the same result but that might help if there are more than 2 columns to gather.
new_tidy_data<- wide_data %>% gather(year, fertility, -country)
class(new_tidy_data$year)
head(new_tidy_data)
```

- The fourth argument is a "convert" argument to convert "characters" into "integers". By examining the class of the "year" column in the data gathered above it appears to be a "character" although in the formatted data it was "integer". This is because the gather function assumes the columns contain characters" not "integers" unless the fourth argument is specified. in this case
```{r}
new_tidy_data<- wide_data %>% gather(year, fertility, -country, convert = TRUE)
class(new_tidy_data$year)
head(new_tidy_data)
```

-Now that the data is "tidy", I can generate a plot.
```{r}
new_tidy_data %>% ggplot(aes(year, fertility, color = country)) + geom_point()
```

- If for any reason i want to do the opposite, which is convert "tidy" data into "wide" data i can use the `spread()` function. it takes 2 arguments. First argument is the column that contains the data i want to spread & the second argument is the data i want to use to populate the table.
```{r}
new_wide_data<- new_tidy_data %>% spread(year, fertility)
select(new_wide_data, country, '1960' : '1967')
```

- Now we can work on a "Raw" file data that needs a bit more work to be converted into "tidy" format. As seen below, the problem with this data is that it's "wide" and that the columns first row are the year & the variable type.
```{r}
path<- system.file("extdata", package = "dslabs")
filename<- file.path(path, "life-expectancy-and-fertility-two-countries-example.csv")
raw_dat<- read_csv(filename)
select(raw_dat,1:5)
```

- First step would be to gather the columns as we did above
```{r}
dat<- raw_dat %>% gather(key, value, -country)
head(dat)
```

- Second we need to separate the year & the variable types. To do this we can use the `separate()` function which takes 3 arguments. `separate(1,2,3)` First argument is the name of the column to be separated, Second argument is the names to be used for the new columns & Third argument is the character that separates the variables i want to separate.
```{r}
dat %>% separate(key, c("year", "variable_name"), "_")
```

- The problem with the code above is that it removed the "_" separating the "life_expectancy" variable. In that case i can use the `extra` argument in the `separate()` function so that we can remove the first "_" between the year & the variable type only & keep all other "_" used in the columns.
```{r}
dat %>% separate(key, c("year", "variable_name"), sep = "_", extra = "merge")
```

- This looks better but still i need to have separate columns for each variable. In this case i can use the `spread()` function i used before.
```{r}
dat %>% separate(key, c("year", "variable_name"), sep="_", extra = "merge") %>% spread(variable_name, value)
```

- Lets say we have the data in the format we need but we want to combine data in 2 separate tables. For Example, lets say we want to combine data from the "muders" dataset with data from the "US elections dataset". One of the problems we could encounter would be that some states could be missing from one of the tables. In the example below, this is not the case as both datasets have the same states. We will see how these tables are joined below & then we will create another scenario where some states will be missing and we will see what could be the possible solutions for that situiation.
```{r}
data(murders)
head(murders)
data("polls_us_election_2016")
head(results_us_election_2016)
identical(results_us_election_2016$state, murders$state)
```

```{r}
tab<- left_join(murders, results_us_election_2016, by="state")
head(tab)
```

- Now we will illustrate how rows might be missing from each table we want to join & we will see how we can fix this issue.We will take a few rows from each of the datasets above & we will explore what options do we have for combining.
```{r}
tab1<- slice(murders, 1:6) %>% select(state, population)
tab1
tab2<- slice(results_us_election_2016, c(1:3, 5, 7:8)) %>% select(state, electoral_votes)
tab2
```

- **Option 1** would be to use `left_join()` to add electoral votes to whatever states are in "tab1"
```{r}
left_join(tab1, tab2)       # OR tab1 %>% left_join(tab2)
```

- **Option 2** would be to use `right_join()` to add the states in "tab1" to the electoral votes in "tab2"
```{r}
tab1 %>% right_join(tab2)
```

- **Option 3** would be to use `inner_join()` to keep only the states that have information from both tables & join them
```{r}
inner_join(tab1, tab2)
```

- **Option 4** would be to use `full_join()` to keep both tables as they are including the NA's & join them.
```{r}
full_join(tab1, tab2)
```

- **Option 5** would be to use `semi_join()` to keep only the data in tab1 that has corresponding data in tab2 *WITHOUT JOINING THEM*
```{r}
semi_join(tab1, tab2)
```

- **Option 6** would be to use `anti_join()` to keep in formation in tab1 that has NO corresponding data in tab2 *WITHOUT JOINING THEM*
```{r}
anti_join(tab1, tab2)
```

- Another way to combine data together is `bind_cols()` or `bind_rows()`. This "bind" function doesn't match data in either data frame to the other, it just add the columns or rows from one data frame to the other without any kind of matching. We will get an error only of the data frames or datasets i'm trying to combine don't match in length.Those are functions of the dplyr package & they produce the output in a "tibble" form. There are equivilant functions for those in base R `cbind()` & `rbind()` but those produce data frames or matrices as output.
```{r}
bind_cols(a=1:3, b=3:5)
```
```{r}
# Using the same functions to bind data frames
tab1<- tab[, 1:3]
tab2<- tab[, 4:6]
tab3<- tab[, 7:9]
new_tab<- bind_cols(tab1, tab2, tab3)
head(new_tab)
```

```{r}
tab1<- tab[1:2,]
tab2<- tab[3:4,]
bind_rows(tab1, tab2)
```

- There are other functions or "set operators" that perform operations on data. Those functions can perform the same operations of data frames when "dplyr" package is loaded
```{r}
# Functions applied to simple vectors
intersect (1:10, 6:15)                            # Intersect finds whats common
intersect(c("a", "b", "c"), c("b", "c", "d"))
union(1:10, 6:15)
union(c("a", "b", "c"), c("b", "c", "d"))         # Union combines the data together without duplicates
setdiff(1:10, 6:15)                               # Setdiff tells us the difference between the first argument & the                                                                        second. This function is not symmetric
setdiff(6:15, 1:10)
setequal(1:5, 1:6)                                # Tells us if 2 sets are equal regardless of order
setequal(1:5, 5:1)
```
```{r}
# Functions applied to data frames
tab1<- tab[1:5,]
tab2<- tab[3:7,]
intersect(tab1, tab2)
union(tab1, tab2)                                 # Works when both data frames have the same columns
setdiff(tab1, tab2)
setequal(tab1, tab2)
```

- ## **WEB SCRAPING**

- HTML: Hyper Text Markup Language. "rvest" is the package inside "tidyverse" that reads data from web pages. In the following example we will see how data for the "murders" dataset was imported from Wikipedia.
```{r}
library(rvest)
url<- "https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state"        # NOTE the "" around the URL
h<- read_html(url)
class(h)
h
```

- As we can see above, when we print "h" we can't see much because the data in that web page is stored in "nodes" between "< >". This is how data is stored in tables in an HTML or XML document. XML: General Markup Language.Functions `html_nodes()` & `html_node()` are functions within "rvest" that extracts the data from within the nodes in an HTML document. First one extracts ALL nodes from that type, the second one extracts only the first node it sees from that type. For example, the node type on that page is "table"
```{r}
tab<- h %>% html_nodes("table")
tab<- tab[[2]]                              # I think that's choosing the "wikitable sortable" type.
tab
```

- As we can see above, that's completely useless. So now there's another useful function `html_table()` that converts the data into a data frame.
```{r}
tab<- tab %>% html_table()
class(tab)
head(tab)
```

- Getting Closer.. since the column names are long, we can assign new column names & check again
```{r}
tab <- tab %>% setNames(c("state", "population", "total", "murders", "gun_murders", "gun_ownership", "total_rate", "murder_rate", "gun_murder_rate"))
head(tab)
```

- Now looking deeper into the data, one of the issues that stand out is that the population & the total variables are "characters" instead of numbers. This is because it's very common for web pages to use commas"," as separators for digits as it makes it easier to read. This is solved by the function `parse_number()`
```{r}
class(tab$population)
class(tab$total)
```

- First we need to understand the following the concept. Strings in R are defined by either single quotes' or double quotes". If for example i want to write a string that already includes double quotes like 10" for example, i will need to use single quotes so i won't get an error `s<-'10"'`. In order to check this in R, i can use the function `cat()` in order to display the string as it is
```{r}
# In order to define a string having " i will need to use a single quote'
s<- '10"'
cat(s)
s

# In order to define a string having a ' i will need to use a double quote"
s<- "5'"
cat(s)

# Now what if i want to use both.. What if i want to define a string 5'10". In this case i need to use \

s<- '5\'10"'
cat(s)

s<- "5'10\""
cat(s)

```

- Now returning to work on the "muders_raw" dataset, `stringr` package includes the functions needed to work on tydying the data in that table for example to remove the commas included in numbers. Functions of this package always start with "str_" so i can type the str_ part then hit "tab" & R will show all available functions of that package.
**NOTE:** Refer to page 444 of the text book for a table of str_ functions & their description.
```{r}
# The code below is supposed to show Identify which columns include numbers with ","
identify_commas<- function(x) any(str_detect(x, ","))
tab %>% summarize_all(list(identify_commas))

```

- Now i can use the `str_replace_all()` function to replace all commas with nothing then coerce the values to numeric. But also, as mentioned before, "readr" package includes function `parse_number()` that does the same thing & also coerces the values to numeric. In the example below the values are not identical because when using the `str_replace_all()` function then `as.numeric()` one value gives NA, but is converted normally using the `parse_number()` function.

```{r}
# Now i can use the `str_replace_all()` function to replace commas & coerce the values to be numbers. NOTE: NA is introduced by coercion because one number includes a footnote which is converted normally without problems using the `parse_number()` function.
test_1<- str_replace_all(tab$population, ",", "")
test_1<- as.numeric(test_1)
head(test_1)
```

```{r}
# Same as above but using `parse_number()`. NOT identical as NA was introduced in test_1
test_2<- parse_number(tab$population)
identical(test_1, test_2)
```


```{r}
# Now mutating the whole dataset. `mutate_at() performs the transformation specified to the columns indicated
murders_new<- tab %>% mutate_at(2:3, parse_number)
head(murders_new)
```

- Looking at another example of web scraping, we want to extract the recipe name, total preparation time & list of ingredients from this foodnetwork page <https://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609>. This is made possible by a software called "Selector Gadget" at <https://selectorgadget.com> which allows us to select which CSS selector we need in order to extract whatever information we need on a page that doesn't have a table like the example shown above. 
```{r}
# This determines which selectors we need based on the "Selector Gadget" software
g<- read_html("https://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")
recipe<- g %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time<- g %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients<- g %>% html_nodes(":nth-child(11) .o-Ingredients__a-Ingredient--CheckboxLabel , :nth-child(10) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(9) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(8) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(7) .o-Ingredients__a-Ingredient--CheckboxLabel, .o-Ingredients__a-Ingredient:nth-child(3) .o-Ingredients__a-Ingredient--CheckboxLabel, #mod-recipe-ingredients-1 :nth-child(2) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(6) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(5) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(4) .o-Ingredients__a-Ingredient--CheckboxLabel") %>% html_text()

# Moving on to extract the data we want & create the list
guacamole<- list(Recipe=recipe, Prep_time=prep_time, Ingredients=ingredients)
guacamole
```


```{r}
# Since ALL recipes on that website follow the same general layout, i can create a function that extracts data from that web page & use it to extract data from any recipe.
get_recipe <- function(url){
    a <- read_html(url)
    recipe <- a %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
    prep_time <- a %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
    ingredients <- a %>% html_nodes(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients<- g %>% html_nodes(":nth-child(11) .o-Ingredients__a-Ingredient--CheckboxLabel , :nth-child(10) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(9) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(8) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(7) .o-Ingredients__a-Ingredient--CheckboxLabel, .o-Ingredients__a-Ingredient:nth-child(3) .o-Ingredients__a-Ingredient--CheckboxLabel, #mod-recipe-ingredients-1 :nth-child(2) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(6) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(5) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(4) .o-Ingredients__a-Ingredient--CheckboxLabel") %>% html_text()
    return(list(Recipe = recipe, Prep_time = prep_time, Ingredients = ingredients))
} 
get_recipe("http://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")
```

- Another example that we can work with is the "heights" dataset raw data in the dslabs package "reported_heights". Due to data input issues, the heights column ended up having non-numeric entries & therefore; is classified as character. We can try to coerce the data into numeric but this will result in a lot of NA's
```{r}
library(dslabs)
data("reported_heights")
class(reported_heights$height)
x<- as.numeric(reported_heights$height)
head(x)
sum(is.na(x))

# Now I can filter the results to know which entries resulted in NA after coercion
reported_heights %>% mutate(new_height = as.numeric(height)) %>% filter(is.na(new_height)) %>% head(n=10)
```

- Looking at the results of the code above, we can see that there are several entries that have patterns that can be easily converted to our desired input in inches. Now we can write a function to isolate those entries to plan how we can start converting them to what we want.
```{r}
not_inches<- function(x, smallest=50, tallest=84){
  inches<- suppressWarnings(as.numeric(x))
  ind<- is.na(inches) | inches< smallest | inches > tallest
  ind
}

# Now we can apply the above function to quickly check how many entries have problems. This line of code can also be replaced by the following which will give the same result `problems<- reported_heights %>% filter(not_inches(height)) %>% .$height
problems<- reported_heights %>% filter(not_inches(height)) %>% pull(height)
length(problems)
problems
```

- If we examine "problems" closely, we will see common patterns that we can isolate & fix. Below are a couple of examples then we will study that in detail.
```{r}
# Identify x'y or x'y'' or x'y"
pattern<- "^\\d\\s*'\\s*\\d{1,2}\\.*\\d*'*\"*$"
str_subset(problems, pattern) %>% head(n=10) %>% cat

# Identify x.y or x,y
pattern2<- "^[4-6]\\s*[\\.|,]\\s*([0-9]|10|11)$"
str_subset(problems, pattern2) %>% head(n=10) %>% cat

# Identify cm
ind<- which(between(suppressWarnings(as.numeric(problems))/2.54, 54, 81))
ind<- ind[!is.na(ind)]
problems[ind] %>% head(n=10) %>% cat
```

- Now lets work on detecting entries including "cm" or "inches". Step 1 would identify "cm", then step 2 would be to detect if the input includes cm or inches with OR indicated by "|". This example builds 2 variables : "yes" & "no" with different inputs then we see how we can detect different patterns in those variables.
```{r}
# Step 1:
str_subset(reported_heights$height, "cm")
# Step 2:
yes<- c("180 cm", "70 inches")
no<- c("180", "70''")
s<- c(yes, no)

str_detect(s, "cm|inches")                    # "|" is used to signify "OR"
```

- Now we will have another example where we will detect a certain pattern in a string, this time will be detecting a digit. In R, a digit is signified by \\d. 
```{r}
yes<- c("5", "6", "5'10", "5 feet", "4'11")
no<- c("", ".", "Five", "six")
a<- c(yes, no)
pattern<- "\\d"
str_detect(a, pattern)
```

- `str_view()` shows the first time a digit is identified & `str_view_all()` shows ALL the times a digit is identified
```{r}
str_view(a, pattern)
str_view_all(a, pattern)
```

- Note that while using the regex functions "str_" to detect patterns we always define a "yes" & "no" variables to test if our pattern detection is working correctly. 

- We use character classes denoted by [] to define a series of characters that we need to look for. If we want to look for characters 5 & 6, then we can check the first example. If we want to look for a range of characters then we can check the second example. **NOTE:** In regex, everything is a character, there are no numbers. Therefore, if i enter the regex as[1-20], this will not look for numbers from 1 to 20, but it will look for the "characters" 1,2,0.
```{r}
# Example 1:
str_view(a, "[56]")

# Example 2:
yes<- as.character(4:7)
no<- as.character(1:3)
s<- c(yes, no)
str_detect(s, "[4-7]")
```

- In order so specify a beginning & an end of a pattern we need to use "Anchors". "^" represent the beginning of a string & "$" represent the end. So in order to look for a string containing ONLY ONE digit it will be written as `^\\d$`. To test that pattern:
```{r}
pattern<- "^\\d$"
yes<- c("1", "5", "9")
no<- c("12", "123", " 1", "a4", "b")
b<- c(yes, no)
str_view(b, pattern)
```

- In order to specify how many times a specific digit can occur we would need to enter how many digits we expect our number to be within curly brakets{}. For Example, we know that inches in the heights column could only be wither 1 or 2 digits.. nothing more. In that case the pattern we would be looking for would be `^\\d{1,2}$`.
```{r}
pattern<- "^\\d{1,2}$"
yes<- c("1", "5", "9", "12")
no<- c("123", "a4", "b")
str_view(c(yes, no), pattern)
```

- Now in order to look for the common input pattern of x'y", we can use the following pattern `"^[4-7]'\\d{1,2}\"$"`. This tells us that we are looking for a pattern of digit 4-7 followed by ' followed by 1 or 2 digits followed by ".
```{r}
pattern<- "^[4-7]'\\d{1,2}\"$"
yes<- c("5'7\"", "6'2\"", "5'12\"")
no<- c("6,2\"", "6.2\"", "I am 5'11\"", "3'2\"", "64")
str_detect(yes, pattern)
str_detect(no, pattern)
```

- Returning to the "problems" variable we extracted from the heights column of the reported heights dataset, we want to see how many of those problematic entries fir the pattern above. This shows that only a few entries follow the described pattern.
```{r}
pattern<- "^[4-7]'\\d{1,2}\"$"
sum(str_detect(problems, pattern))
```

- Checking for entries with the words "inches"
```{r}
str_subset(problems, "inches")
```

- Checking for entries using siglw quotes 2 times to denot inches
```{r}
str_subset(problems, "''") %>% cat
```

- In order to solve that we can replace all that with a standard pattern `x'y` where feet is denoted by ' & inches is not denoted by anything. Therefore; we can update the pattern to be `pattern<- "^[4-7]'\\d{1,2}$"`. If we clean up the data to replace all unwated symbols to fit our pattern, we will get more matches to our pattern.
```{r}
pattern<- "^[4-7]'\\d{1,2}$"
problems %>% str_replace("feet|ft|foot", "'") %>% str_replace("inches|in|''|\"", "") %>% str_detect(pattern) %>% sum
```

- Another issue with this data is "space". Space is represented with `\\s` in R. So in order to include a space in our pattern
```{r}
pattern2<- "^[4-7]'\\s\\d{1,2}\"$"
str_subset(problems, pattern2) %>% cat
```

- But the problem with the above is that R will look for a space after feet. If there are no speces then it will not be detected, this is a problem because it means i will have to write two patterns for looking for spaces. But regex has a useful tool which is called "Quantifiers". This is used to quantify or designate if the symboll i'm looking for exists or not. For Example: * after a symbol like \\s will indicate that this pattern will search for a space that could occur Zero OR more instances.
```{r}
yes<- c("AB", "A1B", "A11B", "A111B", "A1111B")
no<- c("A2B", "A21B")
str_detect(yes, "A1*B")
str_detect(no, "A1*B")
```

- Similar Quantifiers include "?" represents none or once, "+" represents one or more. Here's a comparison
```{r}
data.frame(string=c("AB", "A1B", "A11B", "A111B", "A1111B"),
           none_or_more = str_detect(yes, "A1*B"),
           none_or_once = str_detect(yes, "A1?B"),
           once_or_more = str_detect(yes, "A1+B"))
```

- Further impproving our pattern after adding what we've learned above
```{r}
pattern<- "^[4-7]\\s*'\\s*\\d{1,2}$"
problems %>% str_replace("feet|ft|foot", "'") %>% str_replace("inches|in|''|\"", "") %>% str_detect(pattern) %>% sum
```

