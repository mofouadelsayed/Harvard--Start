---
title: "Data Wrangling"
author: "MO"
date: "30/03/2021"
output: html_document
---
```{r include=FALSE}
library(tidyverse)
library(dslabs)
```

- Spreadsheet files have data stored in rows & columns. If that spreadsheet is opened with a normal text editor the "ROWS" will be separated by "Enter" & the columns will be separated by "," or ";" or " ". It may either also contain a header or not. NOTE: Excel sheets can't be viewed with a text editor.

- To know my current working directory in R `getwd()` & to change the working directory `setwd()`. NOTE: it's always better to provide a full path of the directory i want to change into because by default, if i don't do that, the system will look for files in my current working directory. Ex: `setwd("~/Desktop/HarvardX Data Science/6. Data Wrangling")`
```{r}
# For Example: to know the directory in which the "dslabs" package was installed & what files are in that directory i can do the following
path<- system.file("extdata", package = "dslabs")
path
list.files(path)
```

- Now lets say i want to copy one of the above files to my working directory. I will define the file name i want to copy, then i will define the full path it's located in then i'll copy all that into my working directory. Then i can double check if the file was actually copied
```{r}
filename<- "murders.csv"
fullpath<- file.path(path, filename)
fullpath
file.copy(fullpath, getwd())
file.exists(filename)
```

- Tidyverse package includes functions to read data from Excel sheets directly. They are as follows:
1. `read_excel`: auto detects the excel sheet format either "xls" or "xlsx"
2. `read_xls`: reads files with "xls" format
3. `read_xlsx`: reads files with "xlsx" format
*NOTE:* The sheet function in the above functions specify the sheet to read.

- I can use `read_lines("murders.csv", n_max=3) to read the first 3 lines in that file to know to double check if it has a header & if it's comma or semicolon separated.

- Since i'm trying to import or read a "csv" file, i should use the function as below then i can normally use the `head()` function to check the first 6 lines
```{r}
dat<- read_csv(filename)
```
**OR**
```{r}
dat<- read_csv(fullpath)
class(dat)
```
```{r}
head(dat)
```

- All the above read functions are available in base R as well, but they all use "." instead of "_" in the function name & when the data is imported as data frames as opposed to tibbles in the tidyverse functions. and the strings are converted to factors.
```{r}
dat2<- read.csv(filename)
class(dat2)
```

- To import files from the internet i have 2 options. I can either import or read the file directly from the "url" or i can download a copy on my computer.
```{r eval=FALSE, include=FALSE}
url <- "https://raw.githubusercontent.com/rafalab/dslabs/master/inst/extdata/murders.csv"
dat <- read_csv(url)                              # To read the file directly from the url
download.file(url, "murders.csv")                 # To download the file to my computer
```

- Plotting data as we used to do in the "Visualization" module worked so seamlessly because data was in "tidy" format, which means it was suitable for plotting because it was edited from it's raw format in a format that makes it easy to plot. For Example: every data point was in a different row, there was a header, etc.. below we can examine the raw data of the "gapminder" dataset.
```{r}
path <- system.file("extdata", package="dslabs")
filename <- file.path(path,  "fertility-two-countries-example.csv")
wide_data <- read_csv(filename)
select(wide_data, country, `1960`:`1967`)         # To check the first 9 columns
```

- Now we need to reshape the "Raw" or "wide" data into a "tidy" format so that we can work with it in R. there are several functions in the "tidyr" package which is a part of the "tidyverse" package already that can do that. below we will explore some functions of that package.

- First function we can use is the `gather()` function which will help us group all the year columns we have under one column & will add the relevant fertility data under those columns. This function takes 4 arguments `gather(1, 2, 3, 4)`. The third argument will be the columns i want to group under one column or title, The first argument will be that title or column name i want to group all other columns under & finally the second argument will be the data i want to include in that table. 
```{r}
new_tidy_data<- wide_data %>% gather(year, fertility, '1960' : '2015')
head(new_tidy_data)

# Since the ONLY column that was not gathered from the wide_data in the example above was the country,i can alternatively write the code like this. Both give the same result but that might help if there are more than 2 columns to gather.
new_tidy_data<- wide_data %>% gather(year, fertility, -country)
class(new_tidy_data$year)
head(new_tidy_data)
```

- The fourth argument is a "convert" argument to convert "characters" into "integers". By examining the class of the "year" column in the data gathered above it appears to be a "character" although in the formatted data it was "integer". This is because the gather function assumes the columns contain characters" not "integers" unless the fourth argument is specified. in this case
```{r}
new_tidy_data<- wide_data %>% gather(year, fertility, -country, convert = TRUE)
class(new_tidy_data$year)
head(new_tidy_data)
```

-Now that the data is "tidy", I can generate a plot.
```{r}
new_tidy_data %>% ggplot(aes(year, fertility, color = country)) + geom_point()
```

- If for any reason i want to do the opposite, which is convert "tidy" data into "wide" data i can use the `spread()` function. it takes 2 arguments. First argument is the column that contains the data i want to spread & the second argument is the data i want to use to populate the table.
```{r}
new_wide_data<- new_tidy_data %>% spread(year, fertility)
select(new_wide_data, country, '1960' : '1967')
```

- Now we can work on a "Raw" file data that needs a bit more work to be converted into "tidy" format. As seen below, the problem with this data is that it's "wide" and that the columns first row are the year & the variable type.
```{r}
path<- system.file("extdata", package = "dslabs")
filename<- file.path(path, "life-expectancy-and-fertility-two-countries-example.csv")
raw_dat<- read_csv(filename)
select(raw_dat,1:5)
```

- First step would be to gather the columns as we did above
```{r}
dat<- raw_dat %>% gather(key, value, -country)
head(dat)
```

- Second we need to separate the year & the variable types. To do this we can use the `separate()` function which takes 3 arguments. `separate(1,2,3)` First argument is the name of the column to be separated, Second argument is the names to be used for the new columns & Third argument is the character that separates the variables i want to separate.
```{r}
dat %>% separate(key, c("year", "variable_name"), "_")
```

- The problem with the code above is that it removed the "_" separating the "life_expectancy" variable. In that case i can use the `extra` argument in the `separate()` function so that we can remove the first "_" between the year & the variable type only & keep all other "_" used in the columns.
```{r}
dat %>% separate(key, c("year", "variable_name"), sep = "_", extra = "merge")
```

- This looks better but still i need to have separate columns for each variable. In this case i can use the `spread()` function i used before.
```{r}
dat %>% separate(key, c("year", "variable_name"), sep="_", extra = "merge") %>% spread(variable_name, value)
```

- Lets say we have the data in the format we need but we want to combine data in 2 separate tables. For Example, lets say we want to combine data from the "muders" dataset with data from the "US elections dataset". One of the problems we could encounter would be that some states could be missing from one of the tables. In the example below, this is not the case as both datasets have the same states. We will see how these tables are joined below & then we will create another scenario where some states will be missing and we will see what could be the possible solutions for that situiation.
```{r}
data(murders)
head(murders)
data("polls_us_election_2016")
head(results_us_election_2016)
identical(results_us_election_2016$state, murders$state)
```

```{r}
tab<- left_join(murders, results_us_election_2016, by="state")
head(tab)
```

- Now we will illustrate how rows might be missing from each table we want to join & we will see how we can fix this issue.We will take a few rows from each of the datasets above & we will explore what options do we have for combining.
```{r}
tab1<- slice(murders, 1:6) %>% select(state, population)
tab1
tab2<- slice(results_us_election_2016, c(1:3, 5, 7:8)) %>% select(state, electoral_votes)
tab2
```

- **Option 1** would be to use `left_join()` to add electoral votes to whatever states are in "tab1"
```{r}
left_join(tab1, tab2)       # OR tab1 %>% left_join(tab2)
```

- **Option 2** would be to use `right_join()` to add the states in "tab1" to the electoral votes in "tab2"
```{r}
tab1 %>% right_join(tab2)
```

- **Option 3** would be to use `inner_join()` to keep only the states that have information from both tables & join them
```{r}
inner_join(tab1, tab2)
```

- **Option 4** would be to use `full_join()` to keep both tables as they are including the NA's & join them.
```{r}
full_join(tab1, tab2)
```

- **Option 5** would be to use `semi_join()` to keep only the data in tab1 that has corresponding data in tab2 *WITHOUT JOINING THEM*
```{r}
semi_join(tab1, tab2)
```

- **Option 6** would be to use `anti_join()` to keep in formation in tab1 that has NO corresponding data in tab2 *WITHOUT JOINING THEM*
```{r}
anti_join(tab1, tab2)
```

- Another way to combine data together is `bind_cols()` or `bind_rows()`. This "bind" function doesn't match data in either data frame to the other, it just add the columns or rows from one data frame to the other without any kind of matching. We will get an error only of the data frames or datasets i'm trying to combine don't match in length.Those are functions of the dplyr package & they produce the output in a "tibble" form. There are equivilant functions for those in base R `cbind()` & `rbind()` but those produce data frames or matrices as output.
```{r}
bind_cols(a=1:3, b=3:5)
```
```{r}
# Using the same functions to bind data frames
tab1<- tab[, 1:3]
tab2<- tab[, 4:6]
tab3<- tab[, 7:9]
new_tab<- bind_cols(tab1, tab2, tab3)
head(new_tab)
```

```{r}
tab1<- tab[1:2,]
tab2<- tab[3:4,]
bind_rows(tab1, tab2)
```

- There are other functions or "set operators" that perform operations on data. Those functions can perform the same operations of data frames when "dplyr" package is loaded
```{r}
# Functions applied to simple vectors
intersect (1:10, 6:15)                            # Intersect finds whats common
intersect(c("a", "b", "c"), c("b", "c", "d"))
union(1:10, 6:15)
union(c("a", "b", "c"), c("b", "c", "d"))         # Union combines the data together without duplicates
setdiff(1:10, 6:15)                               # Setdiff tells us the difference between the first argument & the                                                                        second. This function is not symmetric
setdiff(6:15, 1:10)
setequal(1:5, 1:6)                                # Tells us if 2 sets are equal regardless of order
setequal(1:5, 5:1)
```
```{r}
# Functions applied to data frames
tab1<- tab[1:5,]
tab2<- tab[3:7,]
intersect(tab1, tab2)
union(tab1, tab2)                                 # Works when both data frames have the same columns
setdiff(tab1, tab2)
setequal(tab1, tab2)
```

- ## **WEB SCRAPING**

- HTML: Hyper Text Markup Language. "rvest" is the package inside "tidyverse" that reads data from web pages. In the following example we will see how data for the "murders" dataset was imported from Wikipedia.
```{r warning=FALSE}
library(rvest)
url<- "https://en.wikipedia.org/wiki/Gun_violence_in_the_United_States_by_state"        # NOTE the "" around the URL
h<- read_html(url)
class(h)
h
```

- As we can see above, when we print "h" we can't see much because the data in that web page is stored in "nodes" between "< >". This is how data is stored in tables in an HTML or XML document. XML: General Markup Language.Functions `html_nodes()` & `html_node()` are functions within "rvest" that extracts the data from within the nodes in an HTML document. First one extracts ALL nodes from that type, the second one extracts only the first node it sees from that type. For example, the node type on that page is "table"
```{r}
tab<- h %>% html_nodes("table")
tab<- tab[[2]]                              # I think that's choosing the "wikitable sortable" type.
tab
```

- As we can see above, that's completely useless. So now there's another useful function `html_table()` that converts the data into a data frame.
```{r}
tab<- tab %>% html_table()
class(tab)
head(tab)
```

- Getting Closer.. since the column names are long, we can assign new column names & check again
```{r}
tab <- tab %>% setNames(c("state", "population", "total", "murders", "gun_murders", "gun_ownership", "total_rate", "murder_rate", "gun_murder_rate"))
head(tab)
```

- Now looking deeper into the data, one of the issues that stand out is that the population & the total variables are "characters" instead of numbers. This is because it's very common for web pages to use commas"," as separators for digits as it makes it easier to read. This is solved by the function `parse_number()`
```{r}
class(tab$population)
class(tab$total)
```

- First we need to understand the following the concept. Strings in R are defined by either single quotes' or double quotes". If for example i want to write a string that already includes double quotes like 10" for example, i will need to use single quotes so i won't get an error `s<-'10"'`. In order to check this in R, i can use the function `cat()` in order to display the string as it is
```{r}
# In order to define a string having " i will need to use a single quote'
s<- '10"'
cat(s)
s

# In order to define a string having a ' i will need to use a double quote"
s<- "5'"
cat(s)

# Now what if i want to use both.. What if i want to define a string 5'10". In this case i need to use \

s<- '5\'10"'
cat(s)

s<- "5'10\""
cat(s)

```

- Now returning to work on the "muders_raw" dataset, `stringr` package includes the functions needed to work on tydying the data in that table for example to remove the commas included in numbers. Functions of this package always start with "str_" so i can type the str_ part then hit "tab" & R will show all available functions of that package.
**NOTE:** Refer to page 444 of the text book for a table of str_ functions & their description.
```{r}
# The code below is supposed to show Identify which columns include numbers with ","
identify_commas<- function(x) any(str_detect(x, ","))
tab %>% summarize_all(list(identify_commas))

```

- Now i can use the `str_replace_all()` function to replace all commas with nothing then coerce the values to numeric. But also, as mentioned before, "readr" package includes function `parse_number()` that does the same thing & also coerces the values to numeric. In the example below the values are not identical because when using the `str_replace_all()` function then `as.numeric()` one value gives NA, but is converted normally using the `parse_number()` function.

```{r}
# Now i can use the `str_replace_all()` function to replace commas & coerce the values to be numbers. NOTE: NA is introduced by coercion because one number includes a footnote which is converted normally without problems using the `parse_number()` function.
test_1<- str_replace_all(tab$population, ",", "")
test_1<- as.numeric(test_1)
head(test_1)
```

```{r}
# Same as above but using `parse_number()`. NOT identical as NA was introduced in test_1
test_2<- parse_number(tab$population)
identical(test_1, test_2)
```


```{r}
# Now mutating the whole dataset. `mutate_at() performs the transformation specified to the columns indicated
murders_new<- tab %>% mutate_at(2:3, parse_number)
head(murders_new)
```

- Looking at another example of web scraping, we want to extract the recipe name, total preparation time & list of ingredients from this foodnetwork page <https://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609>. This is made possible by a software called "Selector Gadget" at <https://selectorgadget.com> which allows us to select which CSS selector we need in order to extract whatever information we need on a page that doesn't have a table like the example shown above. 
```{r}
# This determines which selectors we need based on the "Selector Gadget" software
g<- read_html("https://www.foodnetwork.com/recipes/alton-brown/guacamole-recipe-1940609")
recipe<- g %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
prep_time<- g %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients<- g %>% html_nodes(":nth-child(11) .o-Ingredients__a-Ingredient--CheckboxLabel , :nth-child(10) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(9) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(8) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(7) .o-Ingredients__a-Ingredient--CheckboxLabel, .o-Ingredients__a-Ingredient:nth-child(3) .o-Ingredients__a-Ingredient--CheckboxLabel, #mod-recipe-ingredients-1 :nth-child(2) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(6) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(5) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(4) .o-Ingredients__a-Ingredient--CheckboxLabel") %>% html_text()

# Moving on to extract the data we want & create the list
guacamole<- list(Recipe=recipe, Prep_time=prep_time, Ingredients=ingredients)
guacamole
```


```{r}
# Since ALL recipes on that website follow the same general layout, i can create a function that extracts data from that web page & use it to extract data from any recipe.
get_recipe <- function(url){
    a <- read_html(url)
    recipe <- a %>% html_node(".o-AssetTitle__a-HeadlineText") %>% html_text()
    prep_time <- a %>% html_node(".m-RecipeInfo__a-Description--Total") %>% html_text()
    ingredients <- a %>% html_nodes(".m-RecipeInfo__a-Description--Total") %>% html_text()
ingredients<- g %>% html_nodes(":nth-child(11) .o-Ingredients__a-Ingredient--CheckboxLabel , :nth-child(10) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(9) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(8) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(7) .o-Ingredients__a-Ingredient--CheckboxLabel, .o-Ingredients__a-Ingredient:nth-child(3) .o-Ingredients__a-Ingredient--CheckboxLabel, #mod-recipe-ingredients-1 :nth-child(2) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(6) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(5) .o-Ingredients__a-Ingredient--CheckboxLabel, :nth-child(4) .o-Ingredients__a-Ingredient--CheckboxLabel") %>% html_text()
    return(list(Recipe = recipe, Prep_time = prep_time, Ingredients = ingredients))
} 
get_recipe("http://www.foodnetwork.com/recipes/food-network-kitchen/pancakes-recipe-1913844")
```

- Another example that we can work with is the "heights" dataset raw data in the dslabs package "reported_heights". Due to data input issues, the heights column ended up having non-numeric entries & therefore; is classified as character. We can try to coerce the data into numeric but this will result in a lot of NA's
```{r}
library(dslabs)
data("reported_heights")
class(reported_heights$height)
x<- as.numeric(reported_heights$height)
head(x)
sum(is.na(x))

# Now I can filter the results to know which entries resulted in NA after coercion
reported_heights %>% mutate(new_height = as.numeric(height)) %>% filter(is.na(new_height)) %>% head(n=10)
```

- Looking at the results of the code above, we can see that there are several entries that have patterns that can be easily converted to our desired input in inches. Now we can write a function to isolate those entries to plan how we can start converting them to what we want.
```{r}
not_inches<- function(x, smallest=50, tallest=84){
  inches<- suppressWarnings(as.numeric(x))
  ind<- is.na(inches) | inches< smallest | inches > tallest
  ind
}

# Now we can apply the above function to quickly check how many entries have problems. The below line of code can also be replaced by the following which will give the same result `problems<- reported_heights %>% filter(not_inches(height)) %>% .$height
problems<- reported_heights %>% filter(not_inches(height)) %>% pull(height)
length(problems)
problems %>% as_tibble()
```

- If we examine "problems" closely, we will see common patterns that we can isolate & fix. Below are a couple of examples then we will study in detail.
```{r}
# Identify x'y or x'y'' or x'y"
pattern<- "^\\d\\s*'\\s*\\d{1,2}\\.*\\d*'*\"*$"
str_subset(problems, pattern) %>% head(n=10) %>% cat

# Identify x.y or x,y
pattern2<- "^[4-6]\\s*[\\.|,]\\s*([0-9]|10|11)$"
str_subset(problems, pattern2) %>% head(n=10) %>% cat

# Identify cm
ind<- which(between(suppressWarnings(as.numeric(problems))/2.54, 54, 81))
ind<- ind[!is.na(ind)]
problems[ind] %>% head(n=10) %>% cat
```

- Now lets work on detecting entries including "cm" or "inches". Step 1 would identify "cm", then step 2 would be to detect if the input includes cm or inches with OR indicated by "|". This example builds 2 variables : "yes" & "no" with different inputs then we see how we can detect different patterns in those variables.
```{r}
# Step 1:
str_subset(reported_heights$height, "cm")
# Step 2:
yes<- c("180 cm", "70 inches")
no<- c("180", "70''")
s<- c(yes, no)

str_detect(s, "cm|inches")                    # "|" is used to signify "OR"
```

- Now we will have another example where we will detect a certain pattern in a string, this time will be detecting a digit. In R, a digit is signified by \\d. 
```{r}
yes<- c("5", "6", "5'10", "5 feet", "4'11")
no<- c("", ".", "Five", "six")
a<- c(yes, no)
pattern<- "\\d"
str_detect(a, pattern)
```

- `str_view()` shows the first time a digit is identified & `str_view_all()` shows ALL the times a digit is identified
```{r}
str_view(a, pattern)
str_view_all(a, pattern)
```

- Note that while using the regex functions "str_" to detect patterns we always define a "yes" & "no" variables to test if our pattern detection is working correctly. 

- We use character classes denoted by [] to define a series of characters that we need to look for. If we want to look for characters 5 & 6, then we can check the first example. If we want to look for a range of characters then we can check the second example. **NOTE:** In regex, everything is a character, there are no numbers. Therefore, if i enter the regex as[1-20], this will not look for numbers from 1 to 20, but it will look for the "characters" 1,2,0.
```{r}
# Example 1:
str_view(a, "[56]")

# Example 2:
yes<- as.character(4:7)
no<- as.character(1:3)
s<- c(yes, no)
str_detect(s, "[4-7]")
```

- In order so specify a beginning & an end of a pattern we need to use "Anchors". "^" represent the beginning of a string & "$" represent the end. So in order to look for a string containing ONLY ONE digit it will be written as `^\\d$`. To test that pattern:
```{r}
pattern<- "^\\d$"
yes<- c("1", "5", "9")
no<- c("12", "123", " 1", "a4", "b")
b<- c(yes, no)
str_view(b, pattern)
```

- In order to specify how many times a specific digit can occur we would need to enter how many digits we expect our number to be within curly brakets{}. For Example, we know that inches in the heights column could only be wither 1 or 2 digits.. nothing more. In that case the pattern we would be looking for would be `^\\d{1,2}$`.
```{r}
pattern<- "^\\d{1,2}$"
yes<- c("1", "5", "9", "12")
no<- c("123", "a4", "b")
str_view(c(yes, no), pattern)
```

- Now in order to look for the common input pattern of x'y", we can use the following pattern `"^[4-7]'\\d{1,2}\"$"`. This tells us that we are looking for a pattern of digit 4-7 followed by ' followed by 1 or 2 digits followed by ".
```{r}
pattern<- "^[4-7]'\\d{1,2}\"$"
yes<- c("5'7\"", "6'2\"", "5'12\"")
no<- c("6,2\"", "6.2\"", "I am 5'11\"", "3'2\"", "64")
str_detect(yes, pattern)
str_detect(no, pattern)
```

- Returning to the "problems" variable we extracted from the heights column of the reported heights dataset, we want to see how many of those problematic entries fir the pattern above. This shows that only a few entries follow the described pattern.
```{r}
pattern<- "^[4-7]'\\d{1,2}\"$"
sum(str_detect(problems, pattern))
```

- Checking for entries with the words "inches"
```{r}
str_subset(problems, "inches")
```

- Checking for entries using siglw quotes 2 times to denot inches
```{r}
str_subset(problems, "''") %>% cat
```

- In order to solve that we can replace all that with a standard pattern `x'y` where feet is denoted by ' & inches is not denoted by anything. Therefore; we can update the pattern to be `pattern<- "^[4-7]'\\d{1,2}$"`. If we clean up the data to replace all unwated symbols to fit our pattern, we will get more matches to our pattern.
```{r}
pattern<- "^[4-7]'\\d{1,2}$"
problems %>% str_replace("feet|ft|foot", "'") %>% str_replace("inches|in|''|\"", "") %>% str_detect(pattern) %>% sum
```

- Another issue with this data is "space". Space is represented with `\\s` in R. So in order to include a space in our pattern
```{r}
pattern2<- "^[4-7]'\\s\\d{1,2}\"$"
str_subset(problems, pattern2) %>% cat
```

- But the problem with the above is that R will look for a space after feet. If there are no spaces then it will not be detected, this is a problem because it means i will have to write two patterns for looking for spaces. But regex has a useful tool which is called "Quantifiers". This is used to quantify or designate if the symbol i'm looking for exists or not. For Example: * after a symbol like \\s will indicate that this pattern will search for a space that could occur Zero OR more instances.
```{r}
yes<- c("AB", "A1B", "A11B", "A111B", "A1111B")
no<- c("A2B", "A21B")
str_detect(yes, "A1*B")
str_detect(no, "A1*B")
```

- Similar Quantifiers include "?" represents none or once, "+" represents one or more. Here's a comparison
```{r}
data.frame(string=c("AB", "A1B", "A11B", "A111B", "A1111B"),
           none_or_more = str_detect(yes, "A1*B"),
           none_or_once = str_detect(yes, "A1?B"),
           once_or_more = str_detect(yes, "A1+B"))
```

- Further improving our pattern after adding what we've learned above
```{r}
pattern<- "^[4-7]\\s*'\\s*\\d{1,2}$"
problems %>% str_replace("feet|ft|foot", "'") %>% str_replace("inches|in|''|\"", "") %>% str_detect(pattern) %>% sum
```

- The next large group of problematic entries were of the form x.y, x,y & x y. We need to change all those to our agreed upon format of x'y. But if we use search & replace for all those patterns we will have a problem with several entries like 70.5 which will change to 70'5. We can do this by defining a pattern with groups between () to identify the digits i want to extract. This enables us to extract 5.6 to 5'6 but don't change an entry of 70.2 inches. Below we will see the same pattern defined by both groups & non-groups methods.
```{r}
pattern_without_groups<- "^[4-7],\\d*$"
pattern_with_groups<- "^([4-7]),(\\d*)$"            #NOTE: If i enter space after "," the result will be wrong.
yes <- c("5,9", "5,11", "6,", "6,1")
no <- c("5'9", ",", "2,8", "6.1.1")
s <- c(yes, no)
str_detect(s, pattern_without_groups)
str_detect(s, pattern_with_groups)
```

- Now we can start extracting these entries after just detecting them in the above step. `str_match()` is a function that extracts the values identified in the pattern *defined by the groups* & displays it in 3 separate columns. First column is the original value, Second & third columns are the single digits we identified in the pattern separately. Function `str_extract()` only extracts the values identified by the pattern, not the values defined by the groups.
```{r}
str_match(s, pattern_with_groups)
str_extract(s, pattern_with_groups)
```

- So in order to identify which group to use for a given action, `\\1` identifies  the first group & `\\2` identifies the second & so on. The below code identifies the pattern groups, then replaces those identified pattern groups with the format identified in the replace function last argument specifying to extract the first digit & enter a ' after it then extract the second digit with nothing after it.
```{r}
pattern_with_groups <-  "^([4-7]),(\\d*)$"
yes <- c("5,9", "5,11", "6,", "6,1")
no <- c("5'9", ",", "2,8", "6.1.1")
s <- c(yes, no)
str_replace(s, pattern_with_groups, "\\1'\\2") %>% cat
```

_ Combining what we learned above, we can create a pattern that gets closer to what we need in this step which is convert all the x.y, x,y & x y' patterns in the heights data. This pattern starts normal as looking for a digit between 4&7 followed by space or not, then a comma, dot or space as we indicated, then followed by a space or not then the second digit. The only problem here is the case of 5.25
```{r}
pattern_with_groups<- "^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$"
str_subset(problems, pattern_with_groups) %>% head(n=15) %>% cat
```

- Creating a function to detect entries with problems then applying it to the reported heights dataset
```{r}
not_inches_or_cm <- function(x, smallest = 50, tallest = 84){
    inches <- suppressWarnings(as.numeric(x))
    ind <- !is.na(inches) &
        ((inches >= smallest & inches <= tallest) |
             (inches/2.54 >= smallest & inches/2.54 <= tallest))
    !ind
}

problems <- reported_heights %>% 
  filter(not_inches_or_cm(height)) %>%
  .$height
length(problems)
```

- Now we will convert the problematic entries to the desired pattern then we will see how much was fixed.
```{r}
converted <- problems %>% 
  str_replace("feet|foot|ft", "'") %>%                                  # Convert feet symbols to '
  str_replace("inches|in|''|\"", "") %>%                                # Remove inches symbols
  str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$", "\\1'\\2")           ## Change format

# Show proportion of entries that fit the desired pattern after reformatting
pattern <- "^[4-7]\\s*'\\s*\\d{1,2}$"
index <- str_detect(converted, pattern)
mean(index)                                                             # Shows converted proportion
converted[!index]                                                       # Shows the remaining problems
```
- Examining the remaining problems, we can try to fix the remaining one by one. The first couple of cases are entries with feet only in the format of x or x'.
```{r}
yes<- c("5", "6", "5'")
no<- c("5''", "5'4")
s<- c(yes, no)
str_replace(s, "^([4-7])'?$","\\1'0")
```

- Next challenge is converting entries with decimal places. In that case we will update the pattern to include the decimal places
```{r}
pattern<- "^[4-7]\\s*'\\s*(\\d+\\.?\\d*)$"
```

- Next would be detecting the entries entered as European commas in meters &converting into decimal.
```{r}
yes <- c("1,7", "1, 8", "2, " )
no <- c("5,8", "5,3,2", "1.7")
s <- c(yes, no)
str_replace(s, "^([12])\\s*,\\s*(\\d*)$", "\\1\\.\\2")
```

- We can also trim or remove spaces entered at the beginning or the end of a string using the `str_trim()` function
```{r}
str_trim("5 ' 9 ")
```

- Converting a string to lower case
```{r}
s <- c("Five feet eight inches")
str_to_lower(s)
```

- Now lets combine what's above to see how far we go in terms of clearing the problematic entries
```{r}
convert_format<- function(s){
  s %>%
    str_replace("feet|foot|ft","'") %>%
    str_replace_all("inches|in|''|\"cm|and","") %>%
    str_replace("^([4-7])\\s*[,\\.\\s+]\\s*(\\d*)$","\\1'\\2") %>%
    str_replace("^([56])'?$","\\1'0") %>%
    str_replace("^([12])\\s*,\\s*(\\d*)$","\\1\\.\\2") %>%
    str_trim()
}
```

- Now we can write a function that converts words to numbers
```{r}
library(english)
words_to_numbers<- function(s){
  s<- str_to_lower(s)
  for (i in 0:11)
    s<- str_replace_all(s, words(i), as.character(i))
  s
}
```

- The above code can be replaced by the following
```{r eval=FALSE, include=FALSE}
words_to_numbers <- function(s){
  str_to_lower(s) %>%  
    str_replace_all("zero", "0") %>%
    str_replace_all("one", "1") %>%
    str_replace_all("two", "2") %>%
    str_replace_all("three", "3") %>%
    str_replace_all("four", "4") %>%
    str_replace_all("five", "5") %>%
    str_replace_all("six", "6") %>%
    str_replace_all("seven", "7") %>%
    str_replace_all("eight", "8") %>%
    str_replace_all("nine", "9") %>%
    str_replace_all("ten", "10") %>%
    str_replace_all("eleven", "11")
}
```

- Combining all the above to see what's left from the problematic entries
```{r}
converted<- problems %>% words_to_numbers() %>% convert_format()
remaining_problems<- converted[not_inches_or_cm(converted)]
pattern<- "^[4-7]\\s*'\\s*\\d+\\.?\\d*$"
index<- str_detect(remaining_problems, pattern)
remaining_problems[!index]
```

- Previously we used the separate function to split the values of a certain column into two separate ones. Extract can do the same thing but also extracts those values. It's obvious below that extract works better than separate if patterns become a bit complicated as below.
```{r}
s<- c("5'10", "6'1\"", "5'8inches")
tab<- data.frame(x=s)
tab
tab %>% separate(x, c("feet", "inches", sep="'"))

# Extract
library(tidyr)
tab %>% extract(x,c("feet", "inches"), regex = "(\\d)'(\\d{1,2})")
```

- Putting it all together.. FINALLY !
```{r}
pattern<- "^([4-7])\\s*'\\s*(\\d+\\.?\\d*)$"
smallest<- 50
tallest<- 84
new_heights<- reported_heights %>%
  mutate(original = height, height= words_to_numbers(height) %>% convert_format()) %>%
  extract(height, c("feet", "inches"), regex= pattern, remove = FALSE) %>%
  mutate_at(c("height", "feet", "inches"), as.numeric) %>%
  mutate(guess=12*feet+inches) %>%
  mutate(height= case_when(
    !is.na(height) & between(height, smallest, tallest) ~ height,
    !is.na(height) & between(height/2.54, smallest, tallest) ~ height/2.54,
    !is.na(height) & between(height*100/2.54, smallest, tallest) ~ height*100/2.54,
     !is.na(guess) & inches < 12 & between(guess, smallest, tallest) ~ guess,
    TRUE ~ as.numeric(NA))) %>%
      mutate(height= ifelse(is.na(height) & inches < 12 & between(guess, smallest, tallest), guess, height)) %>%
      select(-guess)
new_heights %>% head(n=20)
```

- To check all converted entries
```{r}
new_heights %>% filter(not_inches(original)) %>% select(original, height) %>% arrange(height) %>% view()
```

- To check shortest students in our dataset
```{r}
new_heights %>% arrange(height) %>% head(n=7)
```

- *String Splitting*. Usually when we import a ".csv" file using `read_csv()` or `read.csv()` R separates values of each row separated by columns into columns. But to illustrate the uses of `str_split()` function we will assume we imported data with a different function `readLines()` that imports data is tables into lines of continuous strings separated by a comma. Then we will try to separate or "split" those strings to make a data frame.
```{r}
filename<- system.file("extdata/murders.csv", package = "dslabs")
lines<- readLines(filename)
lines %>% head()
```

- So if we try to "split" the strings it will look like this which is not what we want
```{r}
x<- str_split(lines, ",")
head(x)

# To sepearate the column names only (headers)
col_names<- x[[1]]
x<- x[-1]                                             # This defines the data imported without the header
```

- In order ti extract the column number in the second argument of the function `map()`. But to force `map()` to return characters i can use `map_chr()` instead & use `map_int()` if i want integers.
```{r}
map(x,1) %>% head

dat<- tibble(map_chr(x,1),
             map_chr(x,2),
             map_chr(x,3),
             map_chr(x,4),
             map_chr(x,5)) %>%
  mutate_all(parse_guess) %>%
  setNames(col_names)
dat %>% head
```

- ALL the above steps could be replaced by the following which resturns a matrix istead of a list
```{r}
x<- str_split(lines, ",", simplify = TRUE)
col_names<- x[1,]
x<- x[-1,]
colnames(x)<- col_names
x %>% as_tibble() %>% mutate_all(parse_guess) %>% head(5)
```

**Extracting Tables from a PDF**

- WE want to extract data from table S1 found on the PDF published on this website <http://www.pnas.org/content/112/40/12349.abstract>. Below are the steps to download the PDF, Save it to my WD, Extract the data & The first step in cleaning the data which is `str_split()`
```{r}
library(pdftools)
library(stringr)
url<- "https://www.pnas.org/content/pnas/suppl/2015/09/16/1510159112.DCSupplemental/pnas.201510159SI.pdf?targetid=nameddest%3DST1"
download.file(url, "Trial.pdf")               # Download PDF file in url to wd
# To download PDF to a Windows system use `download.file(url, "Trial.pdf", mode="wb")
txt<- pdf_text("Trial.pdf")                   # Convert PDF into text
raw_data_research<- txt[2]                    # The item that contains the table in the text file
table<- str_split(raw_data_research, "\n")    # Splits long lines of strings separated by "\n"" into columns
table<- table[[1]]                            # I think the [[]] converts "table" from list to a value
table
```

- By observing the output of the result above, we see that the column names are in the third & fourth entries
```{r}
the_names_1<- table[3]
the_names_1
the_names_2<- table[4]
the_names_2
```

- Looking at `the_names_1` above we notice that we need to separate things. 1.We need to remove all the space before "Applications" & 2. We need to remove everything after the comma.
```{r}
the_names_1<- the_names_1 %>% str_trim()                               #1. Removes ALL spaces before Entries
the_names_1
the_names_1<- the_names_1 %>% str_replace_all(",\\s.", "")             # str_replace only replaces the fisrt column
the_names_1
the_names_1<- the_names_1 %>% str_split("\\s{2,}", simplify = TRUE)    # \\s{2,} represents 2 OR More spaces
the_names_1
```

- Looking at `the_names_2` we need to also remove the leading space & split
```{r}
the_names_2<- the_names_2 %>% str_trim() %>% str_split("\\s+", simplify = TRUE)
the_names_2
```

- Now I need to combine those two headers to match the data in the PDF. The `str_c()` joins two or more vectors into a single character vector. `rep(the_names_1, each=3)` creates 3 duplicates from the header `the_names_1`. The First line of code therefore; duplicates the first header 3 times then it combines it woth the second header excluding the "Discipline" column. The Second line of code then adds the first column back into the table. The Third line of code then changes all to lower case then replaces the space with another"_"
```{r}
the_names<- str_c(rep(the_names_1, each=3), the_names_2[-1], sep = "_")
the_names<- c(the_names_2[1], the_names)
the_names
the_names<- the_names %>% str_to_lower() %>% str_replace_all("\\s", "_")
the_names
```

- Now working on the data in the table itself. When trying to examine the data in "table" & compare it to the PDF, it's found that the data populating the table will be probably in rows 6 to 14. In the first line of code we can trim all spaces & split data whenever there are spaces. Then the second line of code creates a data frame then adds the headers then changes the data populating the table to numbers except for the first column.
```{r}
new_raw_data_research<- table[6:14] %>% str_trim() %>% str_split("\\s{2,}", simplify = TRUE)
new_raw_data_research
new_raw_data_research<- new_raw_data_research %>% data.frame(stringsAsFactors = FALSE) %>% setNames(the_names) %>% mutate_at(-1, parse_number)
new_raw_data_research
new_raw_data_research %>% as_tibble()       # The only difference i observed was it looks better in console !
```

- Another useful function to use in general would be the `recode()` function in the tidyverse package. Function `case_when()` is less efficient as it does the same thing but we will need to it one by one if this variable is repeated more than once in a datset. The `recode()` function changes that categorical variable name throught the whole dataset all at once. Below is an example of a plot from the "gapminder" dataset in which the variable names of the Caribbean countries could be recoded for shorter ones as they take a lot of space in the plot. 
```{r}
library(dslabs)
data("gapminder")
gapminder %>% filter(region=="Caribbean") %>% ggplot(aes(year, life_expectancy, color=country)) + geom_line()
```

- To see how many Variable have long names (>12 characters). The `distinct()` function displays only the distinct values of the variable entered, without it it would display around 180 values all for the same 4 countries. 
```{r}
gapminder %>% filter(region=="Caribbean") %>% filter(str_length(country) >=12) %>% distinct(country)
```

- Now to recode the names so that they're shorter
```{r}
gapminder %>% filter(region=="Caribbean") %>% mutate(country= recode(country,
                                                                     `Antigua and Barbuda`= "Barbuda",
                                                                     `Dominican Republic`= "DR",
                                                                     `St. Vincent and the Grenadines`= "St. Vince",
                                                                     `Trinidad and Tobago`= "Trinidad")) %>% 
  ggplot(aes(year, life_expectancy, color=country)) + geom_line(size=1)
```

**String Processing Exercise Parts 2 & 3**
```{r}
# Questions 2 & 3
schedule<- tibble(day= c("Monday", "Tuesday"), staff=c("Mandy,Chris and laura", "Steve, Ruth and Frank"))
schedule
tidy<- schedule %>% mutate(staff= str_split(staff, ", | and ")) %>% unnest(cols = c(staff))
tidy

# Question 4
gapminder %>% filter(region=="Middle Africa") %>% filter(str_length(country)>=12) %>% distinct(country)
new<- gapminder %>% filter(region=="Middle Africa") %>% mutate(country_short= recode(country,
                                                                                      `Central African Republic`= "CAR",
                                                                                      `Congo, Dem. Rep.`= "DRC",
                                                                                      `Equotorial Guinea`= "EQ"))
new %>% head(n=10)

# Question 5
library(rvest)
library(tidyverse)
library(stringr)
url <- "https://en.wikipedia.org/w/index.php?title=Opinion_polling_for_the_United_Kingdom_European_Union_membership_referendum&oldid=896735054"
tab <- read_html(url) %>% html_nodes("table")
polls <- tab[[5]] %>% html_table(fill = TRUE)
polls<- polls %>% setNames(c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes"))
pattern<- "^\\d{2,}\\.*\\d*%$"
check<- polls %>% filter(str_detect(remain, pattern))
nrow(check)

# Question 6
check<- as.numeric(str_replace(polls$remain, "%", ""))/100    #OR parse_number(polls$remain)/100
```

**Dates & Times**

- Working with dates from the "polls_us_election_2016". the dates in startdate look like strings but they are not, they are dates.
```{r}
library(dslabs)
data("polls_us_election_2016")
polls_us_election_2016$startdate %>% head
class(polls_us_election_2016$startdate)
# When we convert class dates into numbers, they convert into days since epoch date which is 1-January-1970.
check<- as.numeric(polls_us_election_2016$startdate) %>% head
check
```

- Tidyverse includes another package called "lubridate" that deals with dates. It can perform several useful operations on dates as bellow.
```{r}
# Sorting
dates<- sample(polls_us_election_2016$startdate, 10) %>% sort
dates
```

- Extracting Years, Months & Days
```{r}
library(lubridate)
data.frame(date= dates, month= month(dates), day= day(dates), year= year(dates))
```

-Extracting Month labels
```{r}
month(dates, label=TRUE)
```

- Parsers that converts "strings" into dates in the format given
```{r}
x <- c(20090101, "2009-01-02", "2009 01 03", "2009-1-4", "2009-1, 5", "Created on 2009 1 6", "200901 !!! 07")
ymd(x)
```

- Different parsers extract the year, month & day in whatever order i need
```{r}
x <- "09/01/02"
ymd(x)
mdy(x)
ydm(x)
myd(x)
dmy(x)
dym(x)
```

- This package also deals with time, the below functions display times & the different time zones
```{r}
now()
now("GMT")
OlsonNames() %>% head(n=10)                              # Displays ALL Time Zones

now() %>% hour()
now() %>% minute()
now() %>% second()
```

- It can also parse strings into times & datetimes
```{r}
x<- c("12:34:56")
hms(x)
x<- "Nov/2/2012 12:34:56"
mdy_hms(x)
```

**Trump Tweets**

- The `tidytext` package helps us convert free form text into a tidy table. `unnest_tokens()` can be used to extract individual words and other meaningful chunks of text. Sentiment analysis assigns emotions or a positive/negative score to tokens. You can extract sentiments using `get_sentiments()`

- During the 2016 US presidential election, then-candidate Donald J. Trump used his Twitter account as a way to communicate with potential voters. On August 6, 2016 Todd Vaziri tweeted about Trump that "Every non-hyperbolic tweet is from iPhone (his staff). Every hyperbolic tweet is from Android (from him)." Data scientist David Robinson conducted an analysis to determine if data supported this assertion. Here we go through David's analysis to learn some of the basics of text mining. In general, we can extract data directly from Twitter using the rtweet package. However, in this case, a group has already compiled data for us and made it available at <https://www.thetrumparchive.com>

```{r eval=FALSE, include=FALSE}
# This code needs to be fixed as the website containing the tweets has been modified
library(jsonlite)
url <- 'https://drive.google.com/file/d/16wm-2NTKohhcA26w-kaWfhLIGwl_oX95/view'
trump_tweets<- map(2009:2017, ~sprintf(url, .x)) %>%
  map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
  filter(!is_retweet & !str_detect(text, '^"')) %>%
  mutate(created_at = parse_date_time(created_at, orders = "a b! d! H!:M!:S! z!* Y!", tz="EST"))
```

- Using `?trump_tweets` we can see the actual tweets are stored under the column "text", and the device used to compose the tweet is stored under "source"
```{r}
library(dslabs)
data("trump_tweets")
head(trump_tweets)
names(trump_tweets)                                 # Provides the names of the columns in the data frame
trump_tweets %>% select(text) %>% head
trump_tweets %>% count(source) %>% arrange(desc(n))
```

- Now we can use the `extract` function to remove the "Twitter for" part of the source
```{r}
trump_tweets %>% extract(source, "source", "Twitter for(.*)") %>% count(source)
```

- Now we need to focus on the tweets between the dates Trump announced his campaign & election day
```{r}
campaign_tweets <- trump_tweets %>% 
  extract(source, "source", "Twitter for (.*)") %>%
  filter(source %in% c("Android", "iPhone") &
           created_at >= ymd("2015-06-17") & 
           created_at < ymd("2016-11-08")) %>%
  filter(!is_retweet) %>%
  arrange(created_at)
campaign_tweets %>% head(n=10)
```

- We can now use data visualization to explore the possibility that two different groups were tweeting from these devices. For each tweet, we will extract the hour, in the east coast (EST), it was tweeted then compute the proportion of tweets tweeted at each hour for each device.
```{r}
library(scales)
campaign_tweets %>%
  mutate(hour = hour(with_tz(created_at, "EST"))) %>%
  count(source, hour) %>%
  group_by(source) %>%
  mutate(percent = n / sum(n)) %>%
  ungroup %>%
  ggplot(aes(hour, percent, color = source)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(labels = percent_format()) +
  labs(x = "Hour of day (EST)",
       y = "% of tweets",
       color = "")
```

- Now we need to examine those tweets in further detail to see if there's a difference in the language or "tone" used in every device. The `tidytext` package helps us convert free from text into a tidy table. Having the data in this format greatly facilitates data visualization and applying statistical techniques. The main function needed to achieve this is `unnest_tokens()`. A token refers to the units that we are considering to be a data point. The most common tokens will be words, but they can also be single characters, ngrams, sentences, lines or a pattern defined by a regex. The functions will take a vector of strings and extract the tokens so that each one gets a row in the new table. Here is a simple example:
```{r}
library(tidytext)
example <- data_frame(line = c(1, 2, 3, 4), text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
```

- Now let's take tweet #3008 as an example of how to extract text from a tweet. If i want to apply my code to multiple tweets i can define a variable `i<- c(3008, 3009,4000, 2010, 2050)`
```{r message=FALSE}
campaign_tweets$text[3008]
campaign_tweets[3008,] %>% unnest_tokens(word, text) %>% pull(word)               # If i enter[3008] only without "," i get an error
# But the above code strips the important characters from the tweet like @ & #. To put those back we can add the below argument
campaign_tweets[3008,] %>% unnest_tokens(word, text, token= "tweets") %>% pull(word)  
```

-  Removing the Links to pictures then adding the above step to check how both work on a sample tweet (3008) before applying to everything
```{r message=FALSE}
links <- "https://t.co/[A-Za-z\\d]+|&amp;"
campaign_tweets[3008,] %>% mutate(text= str_replace_all(text, links, "")) %>% unnest_tokens(word, text, token = "tweets") %>% pull(word)
```

- Now applying those rules to ALL tweets
```{r message=FALSE}
tweet_words<- campaign_tweets %>% mutate(text= str_replace_all(text, links, "")) %>% unnest_tokens(word, text, token = "tweets")
```

- Now we can apply statistics to tweets like answering questions like "what are the most commonly used words". But we immediately realize that the most common words are the "stop words" like, (the, to, is, and,...) so, we need to filter those out to have meaningful stats
```{r message=FALSE}
tweet_words %>% count(word) %>% arrange(desc(n)) %>% head(n=20)
# Removing "Stop Words"
tweet_words<- campaign_tweets %>% mutate(text= str_replace_all(text, links, "")) %>% unnest_tokens(word, text, token = "tweets") %>% filter(!word %in% stop_words$word)
tweet_words %>% count(word) %>% mutate(word= reorder(word, n)) %>% arrange(desc(n)) %>% head(n=10)
```

- Some of the words above are actually numbers & some are quotes starting with ', we can remove those as well. The pattern for removing the numbers would be `"^\\d+$`
```{r message=FALSE, warning=FALSE}
tweet_words<- campaign_tweets %>% mutate(text= str_replace_all(text, links, "")) %>% unnest_tokens(word, text, token = "tweets") %>% filter(!word %in% stop_words$word & !str_detect(word, "^\\d+$")) %>% mutate(word= str_replace(word, "^'", ""))
```

- Now let's explore the main idea of the case study in which we need to examine the possibility of identifying the difference in the language or "tone" of tweets sent from iPhone vs. Android. NOTE: a "0.5" correction is added to proportions as a lot are going to be only 0
```{r}
# To count the words from each source
android_iphone<- tweet_words %>% count(word, source)

# To create a separate column for each device & fill the rows without data with "0" instead of "NA"
android_iphone<- tweet_words %>% count(word, source) %>% spread(source, n, fill = 0)

# To create a new column "or" with the ratio described to get the proportion of words
android_iphone<- tweet_words %>% count(word, source) %>% spread(source, n, fill = 0) %>% mutate(or= (Android+0.5) / (sum(Android)- Android+0.5) / ((iPhone+0.5) / (sum(iPhone) -iPhone+0.5)))
```

- Now we can display & arrange highest proportion of words used for every device
```{r}
# To display highest ratios for Android
android_iphone %>% arrange(desc(or)) %>% head(n=20)

# To display highest ratios for iPhone
android_iphone %>% arrange(or) %>% head(n=20)

# To display the words with the highest frequency only (words appearing more than 100 times). For Android
android_iphone %>% filter(Android+iPhone > 100) %>% arrange(desc(or)) %>% head(n=10)

# To display the words with the highest frequency only (words appearing more than 100 times). For iPhone
android_iphone %>% filter(Android+iPhone > 100) %>% arrange(or) %>% head(n=10)
```

**Sentiment Analysis** provides a way to provide insight of sentiment. `tidytext` package include lexicons or maps that assign a sentiment to each word. The "bing" lexicon divides words into positive & negative sentiments. The "afinn" lexicon assigns a core between -5 (most negative) to 5 (most positive). The "loughran" & "nrc" lexicons provide further sentiments
```{r}
library(tidytext)
library(textdata)
get_sentiments("bing")                                              # To explore the bing lexicon
get_sentiments("afinn")                                             # To explore the afinn lexicon
get_sentiments("loughran")
get_sentiments("loughran") %>% count(sentiment)                     # To count the words in each sentiment                  
get_sentiments("nrc")
```

- Now we can use the "nrc" lexicon to evaluate sentiments of tweets from different devices. We will use `inner_join()` function to only keep the words associated with sentiments.
```{r}
nrc<- get_sentiments("nrc") %>% select(word, sentiment)             # Define the lexicon i want to use for evaluation
tweet_words %>% inner_join(nrc, by="word") %>% select(source, word, sentiment) %>% sample_n(10)
```

- Now we can do some analysis to explore the frequencies of each sentiment for each device.
```{r}
# Combine the nrc sentiment list to our dataset
sentiment_counts<- tweet_words %>% left_join(nrc, by="word") %>% head(n=10)
sentiment_counts
# Counts every sentiment detected
sentiment_counts<- tweet_words %>% left_join(nrc, by="word") %>% count(source, sentiment)
sentiment_counts
# Creates a separate column for every device
sentiment_counts<- tweet_words %>% left_join(nrc, by="word") %>% count(source, sentiment) %>% spread(source, n)
sentiment_counts
# Replaces the "NA" sentiments with "None"
sentiment_counts<- tweet_words %>% left_join(nrc, by="word") %>% count(source, sentiment) %>% spread(source, n) %>% mutate(sentiment= replace_na(sentiment, replace="None"))
sentiment_counts
```

- Now we can further compute the odds of being in the device: proportion of words with sentiment versus proportion of words without, and then compute the odds ratio comparing the two devices (whatever that is !)
```{r}
sentiment_counts %>% mutate(Android_Prop = Android / (sum(Android) - Android) ,  iPhone_Prop = iPhone / (sum(iPhone) - iPhone), Proportion = Android_Prop/iPhone_Prop) %>% arrange(desc(Proportion))
```

- So we do see some differences and the order is interesting: the largest three sentiments are disgust, anger, and negative! But are these differences just due to chance? How does this compare if we are just assigning sentiments at random? To answer this question we can compute, for each sentiment, an odds ratio and a confidence interval, as defined in Section 15.10. We will add the two values we need to form a two-by-two table and the odds ratio:
```{r}
library(broom)
log_or <- sentiment_counts %>%
  mutate(log_or = log((Android / (sum(Android) - Android)) / 
      (iPhone / (sum(iPhone) - iPhone))),
          se = sqrt(1/Android + 1/(sum(Android) - Android) + 
                      1/iPhone + 1/(sum(iPhone) - iPhone)),
          conf.low = log_or - qnorm(0.975)*se,
          conf.high = log_or + qnorm(0.975)*se) %>%
  arrange(desc(log_or))
log_or
```

- A graphical visualization shows some sentiments that are clearly overrepresented:
```{r}
log_or %>%
  mutate(sentiment = reorder(sentiment, log_or)) %>%
  ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
  geom_errorbar() +
  geom_point(aes(sentiment, log_or)) +
  ylab("Log odds ratio for association between Android and sentiment") +
  coord_flip() 
```

- We see that the disgust, anger, negative, sadness, and fear sentiments are associated with the Android in a way that is hard to explain by chance alone. Words not associated to a sentiment were strongly associated with the iPhone source, which is in agreement with the original claim about hyperbolic tweets.If we are interested in exploring which specific words are driving these differences, we can refer back to our `android_iphone` object:
```{r}
android_iphone %>% inner_join(nrc) %>%
  filter(sentiment == "disgust" & Android + iPhone > 10) %>%
  arrange(desc(or))
```

- Graph
```{r}
android_iphone %>% inner_join(nrc, by = "word") %>%
  mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
  mutate(log_or = log(or)) %>%
  filter(Android + iPhone > 10 & abs(log_or)>1) %>%
  mutate(word = reorder(word, log_or)) %>%
  ggplot(aes(word, log_or, fill = log_or < 0)) +
  facet_wrap(~sentiment, scales = "free_x", nrow = 2) + 
  geom_bar(stat="identity", show.legend = FALSE) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

**Dates, Times & Text Mining Assessment**
- Questions Part 1
```{r}
# Question 3
check<- brexit_polls %>%  filter(startdate >= ymd("2016-04-01") & startdate<= ymd("2016-04-30")) 
#OR
check<- brexit_polls %>%  mutate(Month= month(startdate,label = TRUE)) %>% filter(Month=="Apr") 
check
# Website Answer
sum(month(brexit_polls$startdate) == 4)
# Second Part
check2<- brexit_polls %>% mutate(enddate= round_date(enddate, "week")) %>% filter(enddate== "2016-06-12")
check2
#Website Answer
sum(round_date(brexit_polls$enddate, unit = "week") == "2016-06-12")

# Question 4
check3<- brexit_polls %>% mutate(day= weekdays(enddate)) %>% count(day)
#Website Answer
table(weekdays(brexit_polls$enddate))

# Question 5
check4<- movielens %>% mutate(timestamp= as_datetime(movielens$timestamp), timestamp= round_date(timestamp, "year")) %>% count(timestamp)
check4
# Website Answer
dates <- as_datetime(movielens$timestamp)
reviews_by_year <- table(year(dates))     # Count reviews by year
names(which.max(reviews_by_year))         # Name of year with most reviews

# Second Part
check4<- movielens %>% mutate(timestamp= as_datetime(movielens$timestamp), Year= year(timestamp), Hour= hour(timestamp)) %>% count(Year) %>% arrange(desc(n))
check4
check4<- movielens %>% mutate(timestamp= as_datetime(movielens$timestamp), Year= year(timestamp), Hour= hour(timestamp)) %>% count(Hour) %>% arrange(desc(n))
check4
# Website Answer
reviews_by_hour <- table(hour(dates))    # Count reviews by hour
names(which.max(reviews_by_hour))        # Name of hour with most reviews
```

- Questions Part 2
```{r}
# Question 6
library(gutenbergr)
library(stringr)
library(tidytext)

check<- filter(gutenberg_metadata, str_detect(gutenberg_metadata$title, "Pride and Prejudice"))
check
# Website Answer
#gutenberg_metadata %>% filter(str_detect(title, "Pride and Prejudice"))

# Question 7
gutenberg_works(title== "Pride and Prejudice")
# Website Answer
#gutenberg_works(title == "Pride and Prejudice")$gutenberg_id

# Question 8
words<- gutenberg_download(gutenberg_id = 1342)
words<- words %>% unnest_tokens(words, text)
# Website Answer
#book <- gutenberg_download(1342)
#words <- book %>% unnest_tokens(word, text)
#nrow(words)

# Question 9
words<- words %>% filter(!words %in% stop_words$word)
words %>% head(n=20)
# Website Answer
#words <- words %>% anti_join(stop_words)
#nrow(words)

# Question 10
words<- words %>% filter(!words %in% stop_words$word & !str_detect(words, "(^\\d+)|(\\d+th)$"))
# Website Answer
#words <- words %>% filter(!str_detect(word, "\\d"))
#nrow(words)

# Question 11
check1<- words %>% count(words)
check1<- filter(check1, n>100) %>% arrange(desc(n))

# Website Answer
#words %>% count(word) %>% filter(n > 100) %>% nrow()
#words %>% count(word) %>% top_n(1, n) %>% pull(word)
#words %>% count(word) %>% top_n(1, n) %>% pull(n)

# Question 12
# Had to rename the column "words" to "word" so it matches the column in "affin_sentiments" to permit inner_join
afinn<- get_sentiments("afinn")
words<- words %>% setNames(c("gutenberg_id","word"))               
afinn_sentiments<- words %>% inner_join(afinn, by="word")
positive<- afinn_sentiments %>% filter(value> 0) %>% count()       
# Divide the outcome by the total in afinn_sentiments to get proportion 
score4<- afinn_sentiments %>% filter(value=="4") %>% count()

# Website Answer
#afinn_sentiments <- inner_join(afinn, words)
#nrow(afinn_sentiments)
#mean(afinn_sentiments$value > 0)
#sum(afinn_sentiments$value == 4)
```


**Puerto Rico Hurricane Mortality Assessment**
```{r}
list.files(system.file("extdata",package = "dslabs"))                                          # List files in dslabs package
fn<- system.file("extdata", "RD-Mortality-Report_2015-18-180531.pdf" , package = "dslabs")     # Define the file i will work on
system2("open", args= fn)                                                                      # Open the file

# In order to start working on that file, i need to copy it to my working drive first then start reading it with the tools
file.copy(fn, getwd())
library(pdftools)
txt<- pdf_text("RD-Mortality-Report_2015-18-180531.pdf")
library(stringr)
raw<- txt[9]
x<- str_split(raw, "\n")
s<- x[[1]]
s<- str_trim(s)
s[1]
#OR header_index<- s %>% str_which("2015") then header_index<- header_index[1]
header_index<- s[2]                                                     
header_index
temp<- str_split(header_index, "\\s+", simplify = TRUE) 
month<- temp[1]
month
header<- temp[-1]
header
header[3]

tail_index<- s[35]
#OR tail_index<- str_which(s, "Total)

n<- str_count(s, "\\d+")
sum(n==1)

out<- c(1:2, which(n==1), 35:length(s))
s<- s[-out]

s <- str_remove_all(s, "[^\\d\\s]")
s <- str_split_fixed(s, "\\s+", n = 6)[,1:5]
s

tab<- s
colnames(tab)<- c("Day", header)
tab<- tab %>% as_data_frame() %>% mutate_all(parse_number)
tab
mean(tab$`2015`)
mean(tab$`2016`)
mean(tab$`2017`[1:19])
mean(tab$`2017`[20:30])

tab<- tab %>% gather(year, deaths, -Day) %>% mutate(deaths= as.numeric(deaths))
library(ggplot2)
tab %>% ggplot(aes(Day, deaths, color= year)) + geom_line(size=1) + geom_vline(xintercept = 19, color= "red")
```

**Exploring On My Own**
- I'll try to import data from my weekly GR Iventory report to Rstudio & Analyze
```{r}
# To list the files in a specific location. NOTICE the "\", if i used the oppsite one it won't work
list.files("C:/Users/Mohamed.Elsayad/Desktop")

# Then to copy the desired file into my WD
file_path<- ("C:/Users/Mohamed.Elsayad/Desktop/GR Weekly Inventory 12-Apr-21.pdf")
file.copy(file_path, getwd())
raw_inventory<- pdf_text("GR Weekly Inventory 12-Apr-21.pdf")
raw_inventory<- str_split(raw_inventory, "\n")
report_date<- raw_inventory[[1]]
report_date<- report_date[1]
report_date<- str_extract(report_date, "[A-Za-z]{3,8}\\s+\\d+,\\s+\\d+")
report_title<- raw_inventory[[1]]
report_title<- report_title[2]
report_title<- str_trim(report_title)
header_1<- raw_inventory[[1]]
header_1<- header_1[4]
header_1<- str_extract(header_1, c("Product Information", "In Process Inventory"))
header_2<- raw_inventory[[1]]
header_2<- header_2[5]
header_2<- str_extract(header_2, c("Prime", "Fin", "Held", "Met"))
header_3<- raw_inventory[[1]]
header_3<- header_3[6]
header_3<- str_split(header_3, "\\s{2,}+")
body_1<- raw_inventory[[1]]
body_1<- body_1[7:33]
check<- str_split_fixed(body_1, "\\s{2,}+", n=13) # Need to figue out how to keep the long sizes as 1 column

```

 